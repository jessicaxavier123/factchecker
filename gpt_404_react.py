# -*- coding: utf-8 -*-
"""GPT 404 React.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xV4XESqbgLPzJzLEhqpOetF2OXC6rbTx
"""

!pip install jsonlines
!pip install langchain
!pip install boto3
!pip install python-dotenv
!pip install langchain-community
!pip install wikipedia

#git remote add origin https://github.com/jessicaxavier123/factchecker.git
#git branch -M main
#git push -u origin main

import jsonlines
import boto3
import openai
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.chains import SimpleSequentialChain
from langchain.llms.bedrock import Bedrock
from dotenv import load_dotenv, find_dotenv

_ = load_dotenv(find_dotenv()) # read local .env file

from langchain.agents import load_tools, initialize_agent, AgentType

def get_llm(model_id='gpt-4o-mini'):
    llm = ChatOpenAI(model_name = model_id, temperature = 0.7, openai_api_key="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")
    return llm

def generate_and_print(llm, q, label):
    print(f'Inside generate_and_print: q = {q}')
    tools = load_tools(["wikipedia"], llm=llm)
    agent = initialize_agent(tools, llm,
                             agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
                             verbose=True,
                             handle_parsing_errors=True,
                             agent_kwargs={})

    input = """Here is a statement:
    {statement}
    Is this statement is correct? You can use tools to find information if needed.
    The final response is FALSE if the statement is FALSE. Otherwise, TRUE."""

    answer = agent.run(input.format(statement=q))

    return answer

def read_questions(llm):
    file='./knowledge_qa_test.jsonl'
    with jsonlines.open(file,'r') as json_f:
        for data in json_f:
            prompt = data.get("prompt", "")
            response = data.get("response", "")
            claims = data.get("claims", [])
            label = data.get("label", "")
            entry_point = data.get("entry_point", "")

            print("Prompt:", prompt)
            print("Response:", response)
            print("Claims:", claims)
            print("label:", label)
            print("entry_point:", entry_point)
            print("\n")
            generate_and_print(llm, response, label)

def main():

    llm = get_llm(model_id="gpt-4o-mini")

    read_questions(llm)

if __name__ == "__main__":
    main()